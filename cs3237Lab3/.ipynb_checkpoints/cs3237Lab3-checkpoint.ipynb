{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3237 Lab 3 Introduction to Deep Learning\n",
    "\n",
    "\n",
    "| Student Number: | Name:                   |\n",
    "|:----------------|:------------------------|\n",
    "| A0242607J     | Mitchell Kok Ming En |\n",
    "| A0196650X     | Jordan Yoong Jia En  |\n",
    "\n",
    "\n",
    "\n",
    "## 0. Special Note\n",
    "\n",
    "Due to changes in Keras and Tensorflow, the code provided in this lab may break with the most recent versions of Tensorflow or Keras. In such event, replace all \"keras\" with \"tensorflow.keras\". E.g. change \"from keras.layers import Dense\" to \"from tensorflow.keras.layers import Dense\".\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "We will achieve the following objectives in this lab:\n",
    "\n",
    "    1. An understanding of the practical limitations of using dense networks in complex tasks\n",
    "    2. Hands-on experience in building a deep learning neural network to solve a relatively complex task.\n",
    "    \n",
    "As this lab is more challenging than the previous labs, please work in teams of two persons. Please use the respective categories in the LumiNUS Forum under the \"Labs\" Heading to find a partner within your own group.\n",
    "\n",
    "Each step may take a long time to run. You and your partner may want to work out how to do things simultaneously, but please do not miss out on any learning opportunities.\n",
    "\n",
    "\n",
    "## 2. Submission Instructions\n",
    "\n",
    "Please work together as a team of 2 to complete this lab. You will need to submit ONE copy of this notebook per team, but please fill in the names of both team members above. This lab is worth 55 marks.\n",
    "\n",
    "**DO NOT SUBMIT MORE THAN ONE COPY OF THIS LAB!**\n",
    "\n",
    "### 2.1 SUBMISSION INSTRUCTIONS\n",
    "\n",
    "Please submit this completed Jupyter Notebook (cs3237lab2.ipynb) to the Files -> Labs -> Lab Submissions -> Lab2 Submissions-> Group Bxx folder by 1.00 PM on the following dates:\n",
    "\n",
    " 1. Group B1: Friday 10 September, 1 pm.\n",
    " 2. Group B2: Monday 13 September, 1 pm.\n",
    " 3. Group B3: Sunday 12 September, 1 pm.\n",
    " \n",
    "### 2.2 LATE SUBMISSION POLICY\n",
    "\n",
    "If you submit between 1 pm and 1.15 pm, a 5 mark penalty will be levied. Submission is strictly not allowed once the folder closes, and you will receive 0 for the lab. NO EXCEPTIONS WILL BE MADE.\n",
    "\n",
    "\n",
    "## 3. Creating a Dense Network for CIFAR-10\n",
    "\n",
    "We will now begin building a neural network for the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50,000 32x32x3 (32x32 pixels, RGB channels) training images and 10,000 testing images (also 32x32x3), divided into the following 10 categories:\n",
    "\n",
    "    1. Airplane\n",
    "    2. Automobile\n",
    "    3. Bird\n",
    "    4. Cat\n",
    "    5. Deer\n",
    "    6. Dog\n",
    "    7. Frog\n",
    "    8. Horse\n",
    "    9. Ship\n",
    "    10. Truck\n",
    "    \n",
    "In the first two parts of this lab we will create a classifier for the CIFAR-10 dataset.\n",
    "\n",
    "### 3.1 Loading the Dataset\n",
    "\n",
    "We begin firstly by creating a Dense neural network for CIFAR-10. The code below shows how we load the CIFAR-10 dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "def load_cifar10():\n",
    "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "    test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "    ret_train_y = to_categorical(train_y,10)\n",
    "    ret_test_y = to_categorical(test_y, 10)\n",
    "    \n",
    "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Explain what the following two  statements do, and where the number \"3072\" came from (2 MARKS):\n",
    "\n",
    "```\n",
    "  train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "  test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "```\n",
    "\n",
    "***ANSWER: \".shape[0]\" returns the number of rows in each set. \".reshape\" then reshapes the datasets into the shape (n, 3072). n is number of rows given by the return value of \".shape[0]\", which is the number of images in each set. 3072 is the number of columns given by the number of pixels in each image (32 * 32 * 3 = 3072, accounting for RGB channels). This process is done for both train_x and test_x.**\n",
    "\n",
    "*FOR GRADER: _______ / 2*\n",
    "\n",
    "### 3.2 Building the MLP Classifier\n",
    "\n",
    "In the code box below, create a new fully connected (dense) multilayer perceptron classifier for the CIFAR-10 dataset. To begin with, create a network with one hidden layer of 1024 neurons, using the SGD optimizer. You should output the training and validation accuracy at every epoch, and train for 50 epochs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.8867 - accuracy: 0.3170 - val_loss: 1.7901 - val_accuracy: 0.3473\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7633 - accuracy: 0.3684 - val_loss: 1.7188 - val_accuracy: 0.3810\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6996 - accuracy: 0.3905 - val_loss: 1.7515 - val_accuracy: 0.3602\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6650 - accuracy: 0.4068 - val_loss: 1.6944 - val_accuracy: 0.3848\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6314 - accuracy: 0.4181 - val_loss: 1.6703 - val_accuracy: 0.3952\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6087 - accuracy: 0.4285 - val_loss: 1.6245 - val_accuracy: 0.4156\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5899 - accuracy: 0.4336 - val_loss: 1.5972 - val_accuracy: 0.4351\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5580 - accuracy: 0.4459 - val_loss: 1.6603 - val_accuracy: 0.4151\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5489 - accuracy: 0.4510 - val_loss: 1.5794 - val_accuracy: 0.4463\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5218 - accuracy: 0.4598 - val_loss: 1.5860 - val_accuracy: 0.4380\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5082 - accuracy: 0.4664 - val_loss: 1.5209 - val_accuracy: 0.4732\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.4904 - accuracy: 0.4715 - val_loss: 1.5530 - val_accuracy: 0.4537\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4602 - accuracy: 0.4827 - val_loss: 1.6134 - val_accuracy: 0.4433\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.4541 - accuracy: 0.4851 - val_loss: 1.5301 - val_accuracy: 0.4661\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.4345 - accuracy: 0.4938 - val_loss: 1.5864 - val_accuracy: 0.4350\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.4243 - accuracy: 0.4953 - val_loss: 1.5055 - val_accuracy: 0.4766\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.4059 - accuracy: 0.5027 - val_loss: 1.5632 - val_accuracy: 0.4551\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3830 - accuracy: 0.5112 - val_loss: 1.5872 - val_accuracy: 0.4550\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3736 - accuracy: 0.5182 - val_loss: 1.5222 - val_accuracy: 0.4710\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3682 - accuracy: 0.5128 - val_loss: 1.5118 - val_accuracy: 0.4749\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3521 - accuracy: 0.5216 - val_loss: 1.5620 - val_accuracy: 0.4563\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3418 - accuracy: 0.5270 - val_loss: 1.5608 - val_accuracy: 0.4630\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3229 - accuracy: 0.5339 - val_loss: 1.5626 - val_accuracy: 0.4637\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3011 - accuracy: 0.5420 - val_loss: 1.5278 - val_accuracy: 0.4845\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2964 - accuracy: 0.5403 - val_loss: 1.6017 - val_accuracy: 0.4601\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2722 - accuracy: 0.5502 - val_loss: 1.6277 - val_accuracy: 0.4502\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2713 - accuracy: 0.5512 - val_loss: 1.5734 - val_accuracy: 0.4712\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2509 - accuracy: 0.5561 - val_loss: 1.6072 - val_accuracy: 0.4520\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2384 - accuracy: 0.5592 - val_loss: 1.5973 - val_accuracy: 0.4757\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2275 - accuracy: 0.5667 - val_loss: 1.6212 - val_accuracy: 0.4683\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2036 - accuracy: 0.5755 - val_loss: 1.5324 - val_accuracy: 0.4911\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1897 - accuracy: 0.5778 - val_loss: 1.6256 - val_accuracy: 0.4750\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1833 - accuracy: 0.5803 - val_loss: 1.6144 - val_accuracy: 0.4673\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1645 - accuracy: 0.5883 - val_loss: 1.5760 - val_accuracy: 0.4832\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1557 - accuracy: 0.5921 - val_loss: 1.6299 - val_accuracy: 0.4850\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1317 - accuracy: 0.5993 - val_loss: 1.6495 - val_accuracy: 0.4701\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1263 - accuracy: 0.6014 - val_loss: 1.5939 - val_accuracy: 0.4861\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1153 - accuracy: 0.6063 - val_loss: 1.6083 - val_accuracy: 0.4953\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1038 - accuracy: 0.6121 - val_loss: 1.6627 - val_accuracy: 0.4848\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0884 - accuracy: 0.6135 - val_loss: 1.7164 - val_accuracy: 0.4847\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0808 - accuracy: 0.6176 - val_loss: 1.6222 - val_accuracy: 0.4881\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0673 - accuracy: 0.6211 - val_loss: 1.6798 - val_accuracy: 0.4867\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0466 - accuracy: 0.6268 - val_loss: 1.6331 - val_accuracy: 0.4951\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0329 - accuracy: 0.6334 - val_loss: 1.6630 - val_accuracy: 0.4920\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0193 - accuracy: 0.6375 - val_loss: 1.7043 - val_accuracy: 0.4890\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0108 - accuracy: 0.6393 - val_loss: 1.7380 - val_accuracy: 0.4894\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9989 - accuracy: 0.6458 - val_loss: 1.7647 - val_accuracy: 0.4766\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9909 - accuracy: 0.6465 - val_loss: 1.7224 - val_accuracy: 0.4853\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9782 - accuracy: 0.6528 - val_loss: 1.7743 - val_accuracy: 0.4874\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9708 - accuracy: 0.6560 - val_loss: 1.7388 - val_accuracy: 0.4903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb27a48e8e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Write your code to build an MLP with one hidden layer of 1024 neurons,\n",
    "with an SGD optimizer. Train for 50 epochs, and output the training and\n",
    "validation accuracy at each epoch.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(1024, input_shape = (3072, ), activation = 'relu'))\n",
    "\n",
    "# Output with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "sgd  = SGD(learning_rate = 0.01, decay = 1e-6, momentum = 0.9)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd,\n",
    "             metrics = 'accuracy')\n",
    "\n",
    "model.fit(x = train_x, y = train_y, shuffle = True, epochs = 50, validation_data = (test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Complete the following table on the design choices for your MLP \n",
    "(3 MARKS):\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            | SGD         | Specified in question |\n",
    "| # of hidden layers   | 1           | Specified in question |\n",
    "| # of hidden neurons  | 1024        | Specified in question |\n",
    "| Hid layer activation | ReLu        | Suitable for classification |\n",
    "| # of output neurons  | 10          | Equal to number of the categories |\n",
    "| Output activation    | Softmax     | Suitable for classification, for use with categorical cross entropy loss function |\n",
    "| lr                   | 0.01 | Initial estimate of suitable learning rate |\n",
    "| momentum             | 0.9 | Helps to control learning overshoot |\n",
    "| decay                | 1e-6 | Amount that learning rate decays by periodically |\n",
    "| loss                 | Categorical Cross Entropy | For multiclass classification |\n",
    "\n",
    "*For TA: ___ / 3* <br>\n",
    "*Code:  ____/ 5* <br>\n",
    "**TOTAL: ____ / 8** <br>\n",
    "\n",
    "#### Question 3:\n",
    "\n",
    "What was your final training accuracy? Validation accuracy? Is there overfitting / underfitting? Explain your answer (5 MARKS)\n",
    "\n",
    "***Training accuracy = 0.6560. Validation Accuracy = 0.4903. There appears to be overfitting as the training accuracy is significantly higher than the validation accuracy.***\n",
    "\n",
    "*FOR GRADER: ______ / 5*\n",
    "\n",
    "### 3.3 Experimenting with the MLP\n",
    "\n",
    "Cut and paste your code from Section 3.2 to the box below (you may need to rename your MLP). Experiment with the number of hidden layers, the number of neurons in each hidden layer, the optimization algorithm, etc. See [Keras Optimizers](https://keras.io/optimizers) for the types of optimizers and their parameters. **Train for 100 epochs.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8243 - accuracy: 0.3497 - val_loss: 1.7937 - val_accuracy: 0.3668\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.6387 - accuracy: 0.4209 - val_loss: 1.6255 - val_accuracy: 0.4360\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5542 - accuracy: 0.4535 - val_loss: 1.5989 - val_accuracy: 0.4329\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4939 - accuracy: 0.4727 - val_loss: 1.5217 - val_accuracy: 0.4607\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4471 - accuracy: 0.4923 - val_loss: 1.4953 - val_accuracy: 0.4743\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4070 - accuracy: 0.5053 - val_loss: 1.4809 - val_accuracy: 0.4738\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3689 - accuracy: 0.5191 - val_loss: 1.4773 - val_accuracy: 0.4706\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3329 - accuracy: 0.5309 - val_loss: 1.4911 - val_accuracy: 0.4790\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3024 - accuracy: 0.5423 - val_loss: 1.4839 - val_accuracy: 0.4677\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.2722 - accuracy: 0.5522 - val_loss: 1.4116 - val_accuracy: 0.4996\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.2460 - accuracy: 0.5633 - val_loss: 1.3443 - val_accuracy: 0.5199\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.2169 - accuracy: 0.5741 - val_loss: 1.4474 - val_accuracy: 0.4866\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.1926 - accuracy: 0.5813 - val_loss: 1.4616 - val_accuracy: 0.4841\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.1644 - accuracy: 0.5924 - val_loss: 1.3525 - val_accuracy: 0.5172\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.1425 - accuracy: 0.6024 - val_loss: 1.3175 - val_accuracy: 0.5314\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.1176 - accuracy: 0.6077 - val_loss: 1.4595 - val_accuracy: 0.5008\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0910 - accuracy: 0.6198 - val_loss: 1.4044 - val_accuracy: 0.5041\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0697 - accuracy: 0.6268 - val_loss: 1.3299 - val_accuracy: 0.5294\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0475 - accuracy: 0.6330 - val_loss: 1.2805 - val_accuracy: 0.5472\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0226 - accuracy: 0.6420 - val_loss: 1.4297 - val_accuracy: 0.4944\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9990 - accuracy: 0.6488 - val_loss: 1.3268 - val_accuracy: 0.5405\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9736 - accuracy: 0.6591 - val_loss: 1.3328 - val_accuracy: 0.5328\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9512 - accuracy: 0.6700 - val_loss: 1.5517 - val_accuracy: 0.4852\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9246 - accuracy: 0.6812 - val_loss: 1.4093 - val_accuracy: 0.5100\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9018 - accuracy: 0.6863 - val_loss: 1.2939 - val_accuracy: 0.5519\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8789 - accuracy: 0.6959 - val_loss: 1.6707 - val_accuracy: 0.4458\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8585 - accuracy: 0.7013 - val_loss: 1.3350 - val_accuracy: 0.5380\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8342 - accuracy: 0.7099 - val_loss: 1.3208 - val_accuracy: 0.5500\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8148 - accuracy: 0.7172 - val_loss: 1.4253 - val_accuracy: 0.5253\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.7897 - accuracy: 0.7272 - val_loss: 1.4427 - val_accuracy: 0.5140\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.7677 - accuracy: 0.7359 - val_loss: 1.4684 - val_accuracy: 0.5185\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.7428 - accuracy: 0.7445 - val_loss: 1.3406 - val_accuracy: 0.5493\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.7201 - accuracy: 0.7531 - val_loss: 1.5870 - val_accuracy: 0.4953\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.6978 - accuracy: 0.7612 - val_loss: 1.3579 - val_accuracy: 0.5532\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.6779 - accuracy: 0.7696 - val_loss: 1.4124 - val_accuracy: 0.5404\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.6572 - accuracy: 0.7756 - val_loss: 1.5772 - val_accuracy: 0.5182\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.6294 - accuracy: 0.7870 - val_loss: 1.4130 - val_accuracy: 0.5477\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.6116 - accuracy: 0.7932 - val_loss: 1.4380 - val_accuracy: 0.5450\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.5885 - accuracy: 0.8010 - val_loss: 1.5853 - val_accuracy: 0.5197\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.5680 - accuracy: 0.8085 - val_loss: 1.4690 - val_accuracy: 0.5508\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.5500 - accuracy: 0.8158 - val_loss: 1.4134 - val_accuracy: 0.5548\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.5265 - accuracy: 0.8221 - val_loss: 1.4651 - val_accuracy: 0.5537\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.5080 - accuracy: 0.8309 - val_loss: 1.5152 - val_accuracy: 0.5476\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.4917 - accuracy: 0.8383 - val_loss: 1.6155 - val_accuracy: 0.5260\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.4675 - accuracy: 0.8457 - val_loss: 1.5728 - val_accuracy: 0.5475\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.4509 - accuracy: 0.8530 - val_loss: 1.6356 - val_accuracy: 0.5341\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.4288 - accuracy: 0.8623 - val_loss: 1.6129 - val_accuracy: 0.5419\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.4157 - accuracy: 0.8636 - val_loss: 1.7246 - val_accuracy: 0.5358\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3956 - accuracy: 0.8698 - val_loss: 1.6542 - val_accuracy: 0.5357\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3768 - accuracy: 0.8790 - val_loss: 1.5677 - val_accuracy: 0.5521\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3604 - accuracy: 0.8835 - val_loss: 1.6677 - val_accuracy: 0.5378\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3463 - accuracy: 0.8910 - val_loss: 1.5907 - val_accuracy: 0.5523\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3276 - accuracy: 0.8968 - val_loss: 1.9544 - val_accuracy: 0.5092\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3089 - accuracy: 0.9053 - val_loss: 1.6279 - val_accuracy: 0.5648\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.2988 - accuracy: 0.9074 - val_loss: 1.6266 - val_accuracy: 0.5625\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.2819 - accuracy: 0.9135 - val_loss: 1.6889 - val_accuracy: 0.5494\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.2685 - accuracy: 0.9174 - val_loss: 1.7897 - val_accuracy: 0.5482\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.2543 - accuracy: 0.9233 - val_loss: 1.9678 - val_accuracy: 0.5259\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2443 - accuracy: 0.9261 - val_loss: 1.7264 - val_accuracy: 0.5481\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2312 - accuracy: 0.9323 - val_loss: 1.8029 - val_accuracy: 0.5511\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2174 - accuracy: 0.9370 - val_loss: 1.7404 - val_accuracy: 0.5597\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.2041 - accuracy: 0.9413 - val_loss: 1.9231 - val_accuracy: 0.5353\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1961 - accuracy: 0.9427 - val_loss: 1.8899 - val_accuracy: 0.5479\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1819 - accuracy: 0.9489 - val_loss: 1.9108 - val_accuracy: 0.5410\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1715 - accuracy: 0.9527 - val_loss: 1.9326 - val_accuracy: 0.5485\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1622 - accuracy: 0.9551 - val_loss: 1.9777 - val_accuracy: 0.5451\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1564 - accuracy: 0.9565 - val_loss: 1.7832 - val_accuracy: 0.5700\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1425 - accuracy: 0.9629 - val_loss: 1.9096 - val_accuracy: 0.5565\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1378 - accuracy: 0.9641 - val_loss: 2.1873 - val_accuracy: 0.5119\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1225 - accuracy: 0.9697 - val_loss: 1.9171 - val_accuracy: 0.5620\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1229 - accuracy: 0.9677 - val_loss: 2.2073 - val_accuracy: 0.5285\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1121 - accuracy: 0.9721 - val_loss: 1.8740 - val_accuracy: 0.5713\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.1024 - accuracy: 0.9759 - val_loss: 2.0775 - val_accuracy: 0.5475\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0962 - accuracy: 0.9783 - val_loss: 1.9424 - val_accuracy: 0.5649\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0905 - accuracy: 0.9794 - val_loss: 2.0639 - val_accuracy: 0.5570\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0819 - accuracy: 0.9822 - val_loss: 1.9394 - val_accuracy: 0.5729\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0777 - accuracy: 0.9839 - val_loss: 2.0069 - val_accuracy: 0.5678\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0719 - accuracy: 0.9850 - val_loss: 2.0320 - val_accuracy: 0.5636\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0683 - accuracy: 0.9855 - val_loss: 2.0133 - val_accuracy: 0.5644\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0610 - accuracy: 0.9880 - val_loss: 2.0636 - val_accuracy: 0.5681\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0607 - accuracy: 0.9884 - val_loss: 2.0429 - val_accuracy: 0.5653\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0518 - accuracy: 0.9909 - val_loss: 2.0439 - val_accuracy: 0.5709\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0484 - accuracy: 0.9921 - val_loss: 2.0888 - val_accuracy: 0.5643\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0460 - accuracy: 0.9926 - val_loss: 2.0576 - val_accuracy: 0.5707\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0403 - accuracy: 0.9944 - val_loss: 2.0883 - val_accuracy: 0.5711\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0388 - accuracy: 0.9943 - val_loss: 2.0991 - val_accuracy: 0.5691\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0366 - accuracy: 0.9948 - val_loss: 2.2158 - val_accuracy: 0.5699\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0342 - accuracy: 0.9951 - val_loss: 2.0917 - val_accuracy: 0.5749\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0310 - accuracy: 0.9960 - val_loss: 2.1185 - val_accuracy: 0.5769\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0285 - accuracy: 0.9966 - val_loss: 2.1639 - val_accuracy: 0.5735\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0280 - accuracy: 0.9968 - val_loss: 2.1972 - val_accuracy: 0.5698\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0260 - accuracy: 0.9972 - val_loss: 2.1681 - val_accuracy: 0.5754\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0254 - accuracy: 0.9970 - val_loss: 2.1910 - val_accuracy: 0.5753\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0219 - accuracy: 0.9979 - val_loss: 2.1987 - val_accuracy: 0.5786\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0201 - accuracy: 0.9982 - val_loss: 2.2122 - val_accuracy: 0.5714\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0191 - accuracy: 0.9982 - val_loss: 2.1888 - val_accuracy: 0.5796\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0189 - accuracy: 0.9981 - val_loss: 2.2594 - val_accuracy: 0.5726\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0181 - accuracy: 0.9984 - val_loss: 2.2349 - val_accuracy: 0.5754\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0163 - accuracy: 0.9989 - val_loss: 2.2438 - val_accuracy: 0.5738\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.0150 - accuracy: 0.9989 - val_loss: 2.2654 - val_accuracy: 0.5784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb27a3cd400>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cut and paste your code from Section 3.2 below, then modify it to get\n",
    "much better results than what you had earlier. E.g. increase the number of\n",
    "nodes in the hidden layer, increase the number of hidden layers,\n",
    "change the optimizer, etc. \n",
    "\n",
    "Train for 100 epochs.\n",
    "\n",
    "\"\"\"\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model2.add(Dense(2048, input_shape = (3072, ), activation = 'relu'))\n",
    "\n",
    "model2.add(Dense(512, activation = 'relu'))\n",
    "\n",
    "# Output with softmax activation\n",
    "model2.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "adam = Adam(learning_rate=0.01)\n",
    "sgd  = SGD(learning_rate = 0.01)\n",
    "model2.compile(loss = 'categorical_crossentropy', optimizer = sgd,\n",
    "             metrics = 'accuracy')\n",
    "\n",
    "model2.fit(x = train_x, y = train_y, shuffle = True, epochs = 100, validation_data = (test_x, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 4:\n",
    "\n",
    "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (3 MARKS)\n",
    "\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            | SGD | Found to be effective in training the model |\n",
    "| # of hidden layers   | 2 | Provide more layers to aid classification |\n",
    "| # neurons(layer1)    | 2048 | More neurons for better classification accuracy |\n",
    "| Hid layer1 activation| ReLu |  Suitable for classification |\n",
    "| # neurons(layer2)    | 512 | More neurons for better classification accuracy |\n",
    "| Hid layer2 activation| ReLu |  Suitable for classification |\n",
    "| # of output neurons  | 10 | Equal to number of the categories |\n",
    "| Output activation    | Softmax | Suitable for classification, for use with categorical cross entropy loss function |\n",
    "| lr                   |0.01 | Found to be effective in initial test |\n",
    "| momentum             |0 | Found to be effective in initial test |\n",
    "| decay                |0 | Found to be effective in initial test |\n",
    "| loss                 | Categorical Cross Entropy | For multiclass classification |\n",
    "\n",
    "*FOR GRADER: _____ / 3 * <br>\n",
    "*CODE: ______ / 5 *<br>\n",
    "\n",
    "***TOTAL: ______ / 8***\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "What is the final training and validation accuracy that you obtained after 150 epochs. Is there considerable improvement over Section 3.2? Are there still signs of underfitting or overfitting? Explain your answer (5 MARKS)\n",
    "\n",
    "***Training accuracy: 0.9989. Validation accuracy: 0.5784. There is significant improvement, but overfitting is still present. This is clear as the training accuracy is very high, but the validation accuracy is much lower.***\n",
    "\n",
    "*FOR GRADER: ______ / 5 *\n",
    "\n",
    "#### Question 6\n",
    "\n",
    "Write a short reflection on the practical difficulties of using a dense MLP to classsify images in the CIFAR-10 datasets. (3 MARKS)\n",
    "\n",
    "***MLPs include too many parameters because they are fully connected, forming a very dense web that results in redundancy and inefficiency when there are many parameters. In the case of image classification, there are many parameters, and this becomes unmanageable and inefficient for large images/sets.***\n",
    "\n",
    "*FOR GRADER: _______ /3*\n",
    "\n",
    "----\n",
    "\n",
    "## 4. Creating a CNN for the MNIST Data Set\n",
    "\n",
    "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in the previous lab. Let's go through each part to see how to do this.\n",
    "\n",
    "### 4.1 Loading the MNIST Dataset\n",
    "\n",
    "As always we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_mnist():\n",
    "    (train_x, train_y),(test_x, test_y) = mnist.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
    "\n",
    "    train_x=train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    \n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "        \n",
    "    train_y = to_categorical(train_y, 10)\n",
    "    test_y = to_categorical(test_y, 10)\n",
    "        \n",
    "    return (train_x, train_y), (test_x, test_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the CNN\n",
    "\n",
    "We will now build the CNN. Unlike before we will create a function to produce the CNN. We will also look at how to save and load Keras models using \"checkpoints\", particularly \"ModelCheckpoint\" that saves the model each epoch.\n",
    "\n",
    "Let's begin by creating the model. We call os.path.exists to see if a model file exists, and call \"load_model\" if it does. Otherwise we create a new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model loads a model from a hd5 file.\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "MODEL_NAME = 'mnist-cnn.hd5'\n",
    "\n",
    "def buildmodel(model_name):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)                                                                                             \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "        model.add(Flatten()) # Question 9\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 7\n",
    "\n",
    "The first layer in our CNN is a 2D convolution kernel, shown here:\n",
    "\n",
    "```\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "```\n",
    "\n",
    "Why is the input_shape set to (28, 28, 1)? What does this mean? What does \"padding = 'same'\" mean? (4 MARKS)\n",
    "\n",
    "***\"input_shape\" informs the model that the input data (images from the MNIST dataset) have the dimensions of 28 * 28 * 1, indicating a 2D image with no colour data. \"padding='same'\" sets the layer outputs to have the same dimensions as its inputs - hence padding is required for the inputs.***\n",
    "\n",
    "*FOR GRADER: ______ / 4*\n",
    "\n",
    "#### Question 8\n",
    "\n",
    "The second layer is the MaxPooling2D layer shown below:\n",
    "\n",
    "```\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "```\n",
    "\n",
    "What other types of pooling layers are available? What does 'strides = 2' mean? (3 MARKS)\n",
    "\n",
    "***Available Layers: MaxPooling1D layer, MaxPooling2D layer, MaxPooling3D layer, AveragePooling1D layer, AveragePooling2D layer, AveragePooling3D layer, GlobalMaxPooling1D layer, GlobalMaxPooling2D layer,  GlobalMaxPooling3D layer, GlobalAveragePooling1D layer, GlobalAveragePooling2D layer, GlobalAveragePooling3D layer. \"strides=2\" informs the model to move the pooling window by 2 units in each dimension for each pooling step.***\n",
    "\n",
    "*FOR GRADER: _____ / 3*\n",
    "\n",
    "\n",
    "#### Question 9\n",
    "\n",
    "What does the \"Flatten\" layer here do? Why is it needed?\n",
    "\n",
    "```\n",
    "        model.add(Flatten()) # Question 9\n",
    "```\n",
    "\n",
    "***The \"Flatten\" layer transforms the input shape to a 1-dimensional array. This ensures the shape of the data fits the shape of the dense layer following the \"Flatten\" layer.***\n",
    "\n",
    "*FOR GRADER: ____ / 2*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### 4.3 Training the CNN\n",
    "\n",
    "Let's now train the CNN. In this example we introduce the idea of a \"callback\", which is a routine that Keras calls at the end of each epoch. Specifically we look at two callbacks:\n",
    "\n",
    "    1. ModelCheckpoint: When called, Keras saves the model to the specified filename.\n",
    "    \n",
    "    2. EarlyStopping: When called, Keras checks if it should stop the training prematurely.\n",
    "    \n",
    "\n",
    "Let's look at the code to see how training is done, and how callbacks are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
    "\n",
    "    model.compile(optimizer=SGD(lr=0.01, momentum=0.7), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    savemodel = ModelCheckpoint(model_name)\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "\n",
    "    print(\"Done. Now evaluating.\")\n",
    "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there isn't very much that is unusual going on; we compile the model with our loss function and optimizer, then call fit, and finally evaluate to look at the final accuracy for the test set.  The only thing unusual is the \"callbacks\" parameter here in the fit function call\n",
    "\n",
    "```\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 10.\n",
    "\n",
    "What does do the min_delta and patience parameters do in the EarlyStopping callback, as shown below? (2 MARKS)\n",
    "\n",
    "```\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "```\n",
    "***\"min_delta\" sets the minimum change required to qualify as an improvement - any change below min_delta will not register as an improvement. \"patience\" sets number of epochs with no improvement after which training will be stopped.***\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Putting it together.\n",
    "\n",
    "Now let's run the code and see how it goes (Note: To save time we are training for only 5 epochs; we should train much longer to get much better results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitchellkok/Documents/CS3237/venv/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.3626 - accuracy: 0.8853 - val_loss: 0.0817 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 01:17:31.646743: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0719 - accuracy: 0.9780 - val_loss: 0.0518 - val_accuracy: 0.9840\n",
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0487 - accuracy: 0.9852 - val_loss: 0.0407 - val_accuracy: 0.9863\n",
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0359 - accuracy: 0.9890 - val_loss: 0.0355 - val_accuracy: 0.9886\n",
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0281 - accuracy: 0.9910 - val_loss: 0.0363 - val_accuracy: 0.9893\n",
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n",
      "Done. Now evaluating.\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0363 - accuracy: 0.9893\n",
      "Test accuracy: 0.99, loss: 0.04\n"
     ]
    }
   ],
   "source": [
    "    (train_x, train_y),(test_x, test_y) = load_mnist()\n",
    "    model = buildmodel(MODEL_NAME)\n",
    "    train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 11.\n",
    "\n",
    "Compare the relative advantages and disadvantages of CNN vs. the Dense MLP that you build in sections 3.2 and 3.3. What makes CNNs better (or worse)? (3 MARKS)\n",
    "\n",
    "***CNNs appear to be more effective at image classification - the CNN achieves higher accuracy with fewer epochs compared to the Dense MLP. This is because images have very high dimensionality, and CNNs are effective in reducing the number of parameters.***\n",
    "\n",
    "*FOR TA: ______ / 3*\n",
    "\n",
    "## 5. Making a CNN for the CIFAR-10 Dataset\n",
    "\n",
    "Now comes the fun part: Using the example above for creating a CNN for the MNIST dataset, now create a CNN in the box below for the MNIST-10 dataset. At the end of each epoch save the model to a file called \"cifar.hd5\" (note: the .hd5 is added automatically for you).\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 12.\n",
    "\n",
    "Summarize your design in the table below (the actual coding cell comes after this):\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            | optimizer=SGD(learning_rate=0.01, momentum=0.7)|                       |\n",
    "| Input shape          | input_shape=(32, 32, 3)| Match the image size for CIFAR-10 (32 * 32 * 3 for RGB)  |\n",
    "| First layer          | model.add(MaxPooling2D(pool_size=(2,2), strides=2)) | Set pool size to 2*2 and stride to 2 |\n",
    "| 2nd, 3rd, 4th layer         | model.add(Conv2D(128, kernel_size=(5,5), activation='relu')) | Convolution layer with kernel window 5*5 to balance accuracy and computational cost|\n",
    "| Add more layers      |            - |                      - |\n",
    "| if needed            |            - |                      - |\n",
    "| Dense layer 1         |Dense(1024, activation='relu')| Hidden layer to help classification |\n",
    "| Dense layer 2         |model.add(Dense(10, activation='softmax'))| Output layer for final classification |\n",
    "\n",
    "\n",
    "*FOR TA:*\n",
    "*Table: ________ / 3* <br>\n",
    "*Code: _________/ 7* <br>\n",
    "**TOTAL: _______ / 10** <br>\n",
    "\n",
    "---\n",
    "\n",
    "***TOTAL: _______ / 55***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 65s 41ms/step - loss: 1.2418 - accuracy: 0.5588 - val_loss: 1.1686 - val_accuracy: 0.5790\n",
      "INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 1.0413 - accuracy: 0.6318 - val_loss: 1.0555 - val_accuracy: 0.6335\n",
      "INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.8980 - accuracy: 0.6859 - val_loss: 0.9518 - val_accuracy: 0.6678\n",
      "INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.7793 - accuracy: 0.7262 - val_loss: 0.8864 - val_accuracy: 0.6935\n",
      "INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 62s 40ms/step - loss: 0.6827 - accuracy: 0.7610 - val_loss: 0.8279 - val_accuracy: 0.7095\n",
      "INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "Done. Now evaluating.\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.8279 - accuracy: 0.7095\n",
      "Test accuracy: 0.71, loss: 0.83\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write your code for your CNN for the CIFAR-10 dataset here. \n",
    "\n",
    "Note: train_x, train_y, test_x, test_y were changed when we called \n",
    "load_mnist in the previous section. You will now need to call load_cifar10\n",
    "again.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "MODEL_NAME = 'cifar.hd5'\n",
    "\n",
    "def load_cifar10():\n",
    "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 32,32,3)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 32,32,3)\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "    ret_train_y = to_categorical(train_y,10)\n",
    "    ret_test_y = to_categorical(test_y, 10)\n",
    "    \n",
    "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
    "\n",
    "\n",
    "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
    "\n",
    "    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.7), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    savemodel = ModelCheckpoint(model_name)\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "\n",
    "    print(\"Done. Now evaluating.\")\n",
    "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))\n",
    "\n",
    "    \n",
    "def buildmodel(model_name):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)                                                                                             \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(32, 32, 3), padding='same')) # Question 7\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "        model.add(Flatten()) # Question 9\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_cifar10()\n",
    "model = buildmodel(MODEL_NAME)\n",
    "train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
