{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3237 Lab 2 - Neural Networks\n",
    "\n",
    "\n",
    "# CS3237 Lab 2 - Neural Networks (MARKING SCHEME)\n",
    "\n",
    "|Student Nunmber|Name                  |\n",
    "|:--------------|:---------------------|\n",
    "| A0242607J     | Mitchell Kok Ming En |\n",
    "| A0196650X     | Jordan Yoong Jia En  |\n",
    "\n",
    "Please work together as a team of 2 to complete this lab. You will need to submit ONE copy of this notebook per team, but please fill in the names of both team members above. This lab is worth 25 marks:\n",
    "\n",
    "**DO NOT SUBMIT MORE THAN ONE COPY OF THIS LAB!**\n",
    "\n",
    "## SUBMISSION INSTRUCTIONS\n",
    "\n",
    "Please submit this completed Jupyter Notebook (cs3237lab2.ipynb) to the Files -> Labs -> Lab Submissions -> Lab2 Submissions-> Group Bxx folder by 1.00 PM on the following dates:\n",
    "\n",
    " 1. Group B1: Friday 3 September, 1 pm.\n",
    " 2. Group B2: Monday 6 September, 1 pm.\n",
    " 3. Group B3: Sunday 5 September, 1 pm.\n",
    " \n",
    "## LATE SUBMISSION POLICY\n",
    "\n",
    "If you submit between 1 pm and 1.15 pm, a 5 mark penalty will be levied. Submission is strictly not allowed once the folder closes, and you will receive 0 for the lab. NO EXCEPTIONS WILL BE MADE.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objectives of this lab are:\n",
    "\n",
    "    1. To familiarize you with how to create dense neural networks using Keras.\n",
    "    2. To familiarize you with how to encode input and output vectors for neural networks.\n",
    "    3. To give you some insight into how hyperparameters like learning rate and momentum affect training.\n",
    "    \n",
    "To save time we will train each experiment only for 50 epochs. This will lead to less than optimal results but is enough for you to make observations.\n",
    "\n",
    "**HINT: YOU CAN HIT SHIFT-ENTER TO RUN EACH CELL. NOTE THAT IF A CELL IS DEPENDENT ON A PREVIOUS CELL, YOU WILL NEED TO RUN THE PREVIOUS CELL(S) FIRST **\n",
    "\n",
    "\n",
    "## 2. The Irises Dataset\n",
    "\n",
    "We will now work again on the Irises Dataset, which we used in Lab 1, for classifying iris flowers into one of three possible types. As before we will consider four factors:\n",
    "\n",
    "    1. Sepal length in cm\n",
    "    2. Sepal width in cm\n",
    "    3. Petal length in cm\n",
    "    4. Petal width in cm\n",
    "\n",
    "In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"First 10 rows of data:\")\n",
    "print(iris.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling the Data\n",
    "\n",
    "We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.\n",
    "\n",
    "In the next section we will investigate what happens if we use unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of SCALED data.\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris.data)\n",
    "X = scaler.transform(iris.data)\n",
    "\n",
    "print(\"First 10 rows of SCALED data.\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding the Targets\n",
    "\n",
    "In Lab 1 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to train the neural network, but we will use \"one-hot\" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:\n",
    "\n",
    "|   Value    |    One-Hot Encoding    |\n",
    "|:----------:|:----------------------:|\n",
    "| 0 | \\[1 0 0\\] |\n",
    "| 1 | \\[0 1 0\\] |\n",
    "| 2 | \\[0 0 1\\] |\n",
    "\n",
    "Keras provides the to_categorical function to create one-hot vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-11 09:26:11.966544: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:11.966618: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(y = iris.target, num_classes = 3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                    random_state = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building our Neural Network\n",
    "\n",
    "Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.\n",
    "\n",
    "The code to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-11 09:26:15.083534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-11 09:26:15.085202: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.085550: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.085906: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.086176: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.086439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.086697: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.086954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.087290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-09-11 09:26:15.087332: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-09-11 09:26:15.088143: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/mitchellkok/Documents/cs3237/venv/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create the neural network\n",
    "nn = Sequential()\n",
    "nn.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "nn.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "sgd = SGD(lr = 0.1, momentum=0.01, nesterov=False)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
    "          metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Neural Network\n",
    "\n",
    "As is usually the case, we can call the \"fit\" method to train the neural network for 50 epochs. We will shuffle the training data between epochs, and provide validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-11 09:26:15.916042: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 1s 81ms/step - loss: 1.0405 - accuracy: 0.4250 - val_loss: 1.0462 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9830 - accuracy: 0.5083 - val_loss: 1.0100 - val_accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9393 - accuracy: 0.6250 - val_loss: 0.9784 - val_accuracy: 0.5667\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8984 - accuracy: 0.6917 - val_loss: 0.9461 - val_accuracy: 0.5667\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8618 - accuracy: 0.6917 - val_loss: 0.9103 - val_accuracy: 0.5667\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8273 - accuracy: 0.6917 - val_loss: 0.8779 - val_accuracy: 0.5667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7990 - accuracy: 0.6917 - val_loss: 0.8422 - val_accuracy: 0.5667\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7673 - accuracy: 0.6917 - val_loss: 0.8032 - val_accuracy: 0.5667\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7355 - accuracy: 0.6917 - val_loss: 0.7691 - val_accuracy: 0.5667\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7076 - accuracy: 0.6917 - val_loss: 0.7475 - val_accuracy: 0.5667\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6831 - accuracy: 0.7000 - val_loss: 0.7345 - val_accuracy: 0.5667\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6571 - accuracy: 0.6917 - val_loss: 0.7047 - val_accuracy: 0.5667\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6371 - accuracy: 0.6917 - val_loss: 0.6888 - val_accuracy: 0.5667\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6154 - accuracy: 0.6917 - val_loss: 0.6587 - val_accuracy: 0.5667\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5984 - accuracy: 0.7333 - val_loss: 0.6505 - val_accuracy: 0.5667\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5775 - accuracy: 0.7083 - val_loss: 0.6272 - val_accuracy: 0.6000\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5609 - accuracy: 0.7167 - val_loss: 0.6041 - val_accuracy: 0.6000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5453 - accuracy: 0.7333 - val_loss: 0.5881 - val_accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5309 - accuracy: 0.7583 - val_loss: 0.5738 - val_accuracy: 0.6000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5228 - accuracy: 0.7833 - val_loss: 0.5677 - val_accuracy: 0.6000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5079 - accuracy: 0.7500 - val_loss: 0.5552 - val_accuracy: 0.6000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4950 - accuracy: 0.7500 - val_loss: 0.5244 - val_accuracy: 0.6667\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4832 - accuracy: 0.7917 - val_loss: 0.5097 - val_accuracy: 0.7667\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4726 - accuracy: 0.8250 - val_loss: 0.5027 - val_accuracy: 0.7667\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4648 - accuracy: 0.8583 - val_loss: 0.5082 - val_accuracy: 0.6333\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4545 - accuracy: 0.8000 - val_loss: 0.4917 - val_accuracy: 0.7000\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4481 - accuracy: 0.8000 - val_loss: 0.4719 - val_accuracy: 0.7667\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4399 - accuracy: 0.8250 - val_loss: 0.4567 - val_accuracy: 0.8000\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4361 - accuracy: 0.8417 - val_loss: 0.4457 - val_accuracy: 0.8000\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4234 - accuracy: 0.9000 - val_loss: 0.4477 - val_accuracy: 0.7667\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4168 - accuracy: 0.8917 - val_loss: 0.4433 - val_accuracy: 0.7667\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4111 - accuracy: 0.8833 - val_loss: 0.4456 - val_accuracy: 0.7667\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4043 - accuracy: 0.8917 - val_loss: 0.4487 - val_accuracy: 0.7667\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3967 - accuracy: 0.9083 - val_loss: 0.4399 - val_accuracy: 0.7667\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3889 - accuracy: 0.8750 - val_loss: 0.4203 - val_accuracy: 0.8000\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3840 - accuracy: 0.8750 - val_loss: 0.4001 - val_accuracy: 0.9333\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3805 - accuracy: 0.9083 - val_loss: 0.4019 - val_accuracy: 0.8000\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3717 - accuracy: 0.9083 - val_loss: 0.3998 - val_accuracy: 0.8000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3701 - accuracy: 0.9250 - val_loss: 0.4006 - val_accuracy: 0.8000\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3612 - accuracy: 0.8917 - val_loss: 0.3839 - val_accuracy: 0.9000\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3650 - accuracy: 0.8917 - val_loss: 0.3704 - val_accuracy: 0.9333\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.3530 - accuracy: 0.9333 - val_loss: 0.3672 - val_accuracy: 0.9333\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3483 - accuracy: 0.9083 - val_loss: 0.3584 - val_accuracy: 0.9333\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3419 - accuracy: 0.9333 - val_loss: 0.3572 - val_accuracy: 0.9333\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.3369 - accuracy: 0.9500 - val_loss: 0.3641 - val_accuracy: 0.9000\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.3339 - accuracy: 0.9333 - val_loss: 0.3692 - val_accuracy: 0.8000\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.3274 - accuracy: 0.9167 - val_loss: 0.3513 - val_accuracy: 0.9333\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3238 - accuracy: 0.9583 - val_loss: 0.3586 - val_accuracy: 0.8667\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3191 - accuracy: 0.9250 - val_loss: 0.3279 - val_accuracy: 0.9667\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.3165 - accuracy: 0.9500 - val_loss: 0.3134 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f46362190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, Y_train, shuffle = True, epochs = 50, \n",
    "      validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. ***(4 MARKS)***\n",
    "\n",
    "**No. Both training accuracy and validation accuracy are high and close in value**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 4_\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Consult the documentation for the SGD optimizer [here](https://keras.io/api/optimizers/sgd/). What does the lr parameter do? ***(1 MARK)***\n",
    "\n",
    "**The lr parameter sets the learning_rate - the hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.**\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "The documentation states that the momentum parameter \"accelerates gradient descent in the relevant direction and dampens oscillations\". Using Google or other means, illustrate what this means. ***(2 MARKS)***\n",
    "\n",
    "**Momentum is an extension to the gradient descent optimization algorithm that allows the search to build inertia in a direction in the search space and overcome the oscillations of noisy gradients and coast across flat spots of the search space. It decreases the number of function evaluations required to reach the optima, or to improve the capability of the optimization algorithm, thus leading to better final results. **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 3_\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 3a\n",
    "\n",
    "We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and validation accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the \"remarks\" column, e.g. \"Progresses steadily\", \"some oscillation\" etc. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below **\n",
    "\n",
    "|  lr    | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:------:|---------------|-----------------|-------------------|\n",
    "|0.01    |0.6917         |0.5667           |Peaks early, then settles with low accuracy                       |\n",
    "|0.1     |0.9250         |0.9667           |Generally steady progression, with some oscillation               |\n",
    "|1.0     |0.9667         |1.0000           |Generally steady progression, with minor oscillation              |\n",
    "|10.0    |0.3417         |0.2000           |Accuracies fluctuate without any improvement                      |\n",
    "|100     |0.3000         |0.2000           |Accuracies fluctuate without any improvement                      |\n",
    "|1000    |0.2750         |0.2000           |Accuracies fluctuate without any improvement                      |\n",
    "|10000   |0.3000         |0.2000           |Accuracies fluctuate without any improvement                      |\n",
    "| 100000 |0.3083         |0.3667           |Accuracies fluctuate without any improvement                      |\n",
    "\n",
    "\n",
    "#### Question 3b\n",
    "\n",
    "Based on your observations above, comment on the effect of small and very large learning rates on the learning. ***(2 MARKS)***\n",
    "\n",
    "**A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, while a learning rate that is too small can get stuck without settling at a good accuracy**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "### 2.5 Using Momentum\n",
    "\n",
    "We will now experiment with the momentum term. To do this:\n",
    "\n",
    "    1. Change the learning rate to 0.1.\n",
    "    2. Set the momentum to 0.1. Note: Do not use the Nesterov parameter - Leave it as False.\n",
    "    \n",
    "Run your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the \"Remarks\" column. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below**\n",
    "\n",
    "| momentum | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:--------:|---------------|-----------------|-------------------|\n",
    "|0.001     |0.9417         |0.9667           |                   |\n",
    "|0.01      |0.9333         |0.9667           |                   |\n",
    "|0.1       |0.9250         |0.9333           |                   |\n",
    "|1.0       |0.9000         |0.9667           |                   |\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Based on your observations above, does the momentum term help in learning? ***(2 MARKS)***\n",
    "\n",
    "**There is minimal impact of momentum in classifying this dataset. The training and validation accuracies are high and very close to each other regardless of momentum value.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Using Raw Unscaled Data\n",
    "\n",
    "We begin by using unscaled X and Y data. The code below will create 120 training samples and 30 testing samples (20% of the total of 150 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unscaled = iris.data\n",
    "Y_raw = iris.target\n",
    "X_utrain, X_utest, Y_utrain, Y_utest = train_test_split(X_unscaled, Y,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "Create a new neural network called \"nn2\" below using a single hidden layer of 100 neurons. Train using the data in X_utrain, X_utest and validate with Y_utrain and Y_utest. Again use the SGD optimizer with a learning rate of 0.1 and no momentum, and train for 50 epochs. ***(3 marks)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 1s 56ms/step - loss: 1.5220 - accuracy: 0.4583 - val_loss: 2.5732 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.4861 - accuracy: 0.4583 - val_loss: 0.6866 - val_accuracy: 0.6000\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6132 - accuracy: 0.7333 - val_loss: 0.5495 - val_accuracy: 0.8000\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5783 - accuracy: 0.7667 - val_loss: 0.5077 - val_accuracy: 0.8667\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5151 - accuracy: 0.8167 - val_loss: 0.5673 - val_accuracy: 0.6000\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4343 - accuracy: 0.8583 - val_loss: 0.6545 - val_accuracy: 0.5667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4391 - accuracy: 0.7750 - val_loss: 0.4364 - val_accuracy: 0.8667\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3911 - accuracy: 0.8750 - val_loss: 0.3824 - val_accuracy: 0.9333\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4264 - accuracy: 0.8250 - val_loss: 1.1854 - val_accuracy: 0.5667\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5964 - accuracy: 0.7000 - val_loss: 0.3749 - val_accuracy: 0.9333\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4632 - accuracy: 0.7000 - val_loss: 0.3652 - val_accuracy: 0.9667\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3611 - accuracy: 0.8667 - val_loss: 0.3455 - val_accuracy: 0.9000\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4486 - accuracy: 0.7250 - val_loss: 0.3449 - val_accuracy: 0.8667\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3688 - accuracy: 0.8167 - val_loss: 0.4001 - val_accuracy: 0.8000\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3575 - accuracy: 0.8417 - val_loss: 0.4024 - val_accuracy: 0.8000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4501 - accuracy: 0.7083 - val_loss: 0.3709 - val_accuracy: 0.8000\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4095 - accuracy: 0.8083 - val_loss: 0.6732 - val_accuracy: 0.6000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3226 - accuracy: 0.8833 - val_loss: 0.5108 - val_accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2897 - accuracy: 0.9083 - val_loss: 0.5621 - val_accuracy: 0.6000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3320 - accuracy: 0.8000 - val_loss: 0.2865 - val_accuracy: 0.9667\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2622 - accuracy: 0.9333 - val_loss: 0.2744 - val_accuracy: 0.9000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3010 - accuracy: 0.8917 - val_loss: 0.3904 - val_accuracy: 0.7333\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2306 - accuracy: 0.9750 - val_loss: 0.2613 - val_accuracy: 0.9000\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2429 - accuracy: 0.8917 - val_loss: 0.2532 - val_accuracy: 0.9667\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2166 - accuracy: 0.9417 - val_loss: 0.5319 - val_accuracy: 0.6333\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.6066 - accuracy: 0.6667 - val_loss: 1.3449 - val_accuracy: 0.5667\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4601 - accuracy: 0.8333 - val_loss: 0.4147 - val_accuracy: 0.7000\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2735 - accuracy: 0.9167 - val_loss: 0.2755 - val_accuracy: 0.9667\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2548 - accuracy: 0.9000 - val_loss: 0.2453 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2325 - accuracy: 0.9167 - val_loss: 0.3112 - val_accuracy: 0.8333\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4363 - accuracy: 0.8083 - val_loss: 0.4016 - val_accuracy: 0.7333\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2646 - accuracy: 0.8917 - val_loss: 0.5926 - val_accuracy: 0.8000\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2868 - accuracy: 0.8833 - val_loss: 0.2156 - val_accuracy: 0.9333\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1750 - accuracy: 0.9667 - val_loss: 0.2587 - val_accuracy: 0.8667\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3211 - accuracy: 0.8500 - val_loss: 0.9648 - val_accuracy: 0.6000\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3915 - accuracy: 0.8167 - val_loss: 0.4593 - val_accuracy: 0.7333\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2301 - accuracy: 0.9000 - val_loss: 0.2026 - val_accuracy: 0.9333\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1771 - accuracy: 0.9667 - val_loss: 0.3036 - val_accuracy: 0.8000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5778 - accuracy: 0.7000 - val_loss: 0.4032 - val_accuracy: 0.7333\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1999 - accuracy: 0.9333 - val_loss: 0.2652 - val_accuracy: 0.8667\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1986 - accuracy: 0.9333 - val_loss: 0.4419 - val_accuracy: 0.7333\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1863 - accuracy: 0.9417 - val_loss: 0.2689 - val_accuracy: 0.8667\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.1605 - accuracy: 0.9583 - val_loss: 0.1803 - val_accuracy: 0.9333\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2794 - accuracy: 0.8750 - val_loss: 0.2644 - val_accuracy: 0.8667\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.1512 - accuracy: 0.9500 - val_loss: 0.2153 - val_accuracy: 0.9000\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2496 - accuracy: 0.8583 - val_loss: 0.1828 - val_accuracy: 0.9667\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.1233 - accuracy: 0.9750 - val_loss: 0.2123 - val_accuracy: 0.9000\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2507 - accuracy: 0.9000 - val_loss: 1.0374 - val_accuracy: 0.8000\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.0212 - accuracy: 0.6583 - val_loss: 0.3860 - val_accuracy: 0.7667\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.3035 - accuracy: 0.9083 - val_loss: 0.3081 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f4417bf40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enter your code for Question 5 below. To TA: 3 marks for code.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create the neural network\n",
    "nn2 = Sequential()\n",
    "nn2.add(Dense(100, input_shape = (4, ), activation = 'relu'))   # hidden layer\n",
    "nn2.add(Dense(3, activation = 'softmax'))                       # output layer\n",
    "\n",
    "# Create our optimizer\n",
    "sgd2 = SGD(learning_rate = 0.1)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn2.compile(loss='categorical_crossentropy', optimizer=sgd2, \n",
    "          metrics = 'accuracy')\n",
    "\n",
    "# Train neural networks\n",
    "nn2.fit(X_utrain, Y_utrain, shuffle = True, epochs = 50, \n",
    "      validation_data = (X_utest, Y_utest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Question 5 continues)**\n",
    "\n",
    "Observe the training and validation error. Does not scaling the input affect the training? Why do you think this is so? What is the advantage of scaling? ***(5 MARKS)***\n",
    "\n",
    "**There is minimal impact of scaling on this dataset. Without scaling, the feature with a higher value range starts dominating when calculating distances. However, since the features are sepal/petal lengths and widths, these features are relatively close in value and hence scaling has a much smaller impact for this dataset. In a dataset where the features have vastly different values, there will be a significant bias towards the feature with larger spread (i.e. having larger values), affecting the neural network's learning. In such a case, scaling will help the model learn better by removing the inherent differences in magnitude and achieving faster convergence.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 8_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conclusion\n",
    "\n",
    "In this lab we saw how to create a simple Dense neural network to complete the relatively simple task of learning how to classify irises according to their sepal and petal characteristics. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "***FOR TA ONLY***\n",
    "\n",
    "| Question |  Marks  |\n",
    "|:--------:|:-------:|\n",
    "|1         |     /4  |\n",
    "|2         |     /3  |\n",
    "|3         |     /5  |\n",
    "|4         |     /5  |\n",
    "|5         |     /8  |\n",
    "|Total:    |     /25 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
